"""
FastAPI routes for DDN Multimodal Semantic Search.
"""
import os
import time
import tempfile
import logging
import re
import unicodedata
from typing import List
from datetime import datetime
from pathlib import Path
from fastapi import APIRouter, UploadFile, File, HTTPException, Form
from fastapi.responses import StreamingResponse
from PIL import Image
import io

from app.core.config import settings, storage_config
from app.models.schemas import (
    DDNConfigRequest,
    AWSConfigRequest,
    LocalCacheConfigRequest,
    LocalCacheConfigResponse,
    StorageConfigResponse,
    ConnectionTestResponse,
    ImageUploadResponse,
    VideoUploadResponse,
    DocumentUploadResponse,
    SearchRequest,
    SearchResponse,
    SearchResult,
    BrowseRequest,
    BrowseResponse,
    ObjectInfo,
    HealthResponse,
    MetricsResponse,
    ErrorResponse,
    VideoFrameSearchRequest,
    VideoFrameSearchResponse
)
from app.services.storage import S3Handler
from app.services.ai_models import (
    get_image_analyzer,
    get_video_analyzer,
    get_document_analyzer,
    DEVICE,
    TORCH_AVAILABLE
)
from app.services.bucket_monitor import BucketMonitor


# Routers
config_router = APIRouter(prefix="/config", tags=["Configuration"])
upload_router = APIRouter(prefix="/upload", tags=["Upload"])
search_router = APIRouter(prefix="/search", tags=["Search"])
browse_router = APIRouter(prefix="/browse", tags=["Browse"])
ingestion_router = APIRouter(prefix="/ingestion", tags=["Continuous Ingestion"])
health_router = APIRouter(tags=["Health"])

# Initialize bucket monitor
bucket_monitor = BucketMonitor()


# Configure logger
logger = logging.getLogger(__name__)
def sanitize_for_s3_metadata(text: str) -> str:
    """Sanitize text to be S3 metadata compliant (ASCII only)."""
    if not text:
        return ""

    replacements = {
        '\u201c': '"', '\u201d': '"', '\u2018': "'", '\u2019': "'",
        '\u2014': '-', '\u2013': '-', '\u2012': '-',
        '\u2026': '...', '\u00a0': ' ', '\u2009': ' ',
        '\u2022': '*', '\u2023': '*',
        '\u00ae': '(R)', '\u2122': '(TM)', '\u00a9': '(C)',
    }

    for unicode_char, ascii_char in replacements.items():
        text = text.replace(unicode_char, ascii_char)

    text = unicodedata.normalize('NFKD', text)
    text = text.encode('ascii', 'ignore').decode('ascii')
    text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)

    max_length = 2000
    if len(text) > max_length:
        text = text[:max_length - 3] + '...'

    return text.strip()


# ============== Configuration Routes ==============

@config_router.post("/ddn", response_model=StorageConfigResponse)
async def configure_ddn(config: DDNConfigRequest):
    """Configure DDN INFINIA storage."""
    storage_config.update_ddn_config(
        access_key=config.access_key,
        secret_key=config.secret_key,
        bucket_name=config.bucket_name,
        endpoint_url=config.endpoint_url,
        region=config.region
    )
    return StorageConfigResponse(
        success=True,
        message="DDN INFINIA configuration updated",
        ddn_configured=True,
        aws_configured=bool(storage_config.aws_config.get('access_key'))
    )


@config_router.post("/aws", response_model=StorageConfigResponse)
async def configure_aws(config: AWSConfigRequest):
    """Configure AWS S3 storage."""
    storage_config.update_aws_config(
        access_key=config.access_key,
        secret_key=config.secret_key,
        bucket_name=config.bucket_name,
        region=config.region
    )
    return StorageConfigResponse(
        success=True,
        message="AWS S3 configuration updated",
        ddn_configured=bool(storage_config.ddn_infinia_config.get('access_key')),
        aws_configured=True
    )


@config_router.get("/load")
async def load_config():
    """Load saved configuration from disk."""
    return {
        "success": True,
        "ddn_config": {
            "access_key": storage_config.ddn_infinia_config.get('access_key', ''),
            "secret_key": storage_config.ddn_infinia_config.get('secret_key', ''),
            "bucket_name": storage_config.ddn_infinia_config.get('bucket_name', ''),
            "endpoint_url": storage_config.ddn_infinia_config.get('endpoint_url', ''),
            "region": storage_config.ddn_infinia_config.get('region', 'us-east-1')
        },
        "aws_config": {
            "access_key": storage_config.aws_config.get('access_key', ''),
            "secret_key": storage_config.aws_config.get('secret_key', ''),
            "bucket_name": storage_config.aws_config.get('bucket_name', ''),
            "region": storage_config.aws_config.get('region', 'us-east-1')
        }
    }


@config_router.post("/local-cache", response_model=LocalCacheConfigResponse)
async def configure_local_cache(config: LocalCacheConfigRequest):
    """Configure local cache settings."""
    # Validate paths if enabled
    if config.enabled:
        is_valid, msg = storage_config.validate_config('local_cache')
        if not is_valid:
            # Update first to validate
            storage_config.update_local_cache_config(
                enabled=config.enabled,
                videos_path=config.videos_path,
                embeddings_path=config.embeddings_path
            )
            # Re-validate
            is_valid, msg = storage_config.validate_config('local_cache')
            if not is_valid:
                raise HTTPException(status_code=400, detail=msg)
    
    storage_config.update_local_cache_config(
        enabled=config.enabled,
        videos_path=config.videos_path,
        embeddings_path=config.embeddings_path
    )
    
    return LocalCacheConfigResponse(
        success=True,
        message="Local cache configuration updated successfully",
        enabled=config.enabled,
        videos_path=config.videos_path,
        embeddings_path=config.embeddings_path
    )


@config_router.get("/local-cache", response_model=LocalCacheConfigResponse)
async def get_local_cache_config():
    """Get current local cache configuration."""
    cache_config = storage_config.local_cache_config
    return LocalCacheConfigResponse(
        success=True,
        message="Local cache configuration retrieved",
        enabled=cache_config.get('enabled', False),
        videos_path=cache_config.get('videos_path', ''),
        embeddings_path=cache_config.get('embeddings_path', '')
    )


@config_router.get("/test/{provider}", response_model=ConnectionTestResponse)
async def test_connection(provider: str):
    """Test connection to storage provider."""
    if provider not in ['aws', 'ddn_infinia']:
        raise HTTPException(status_code=400, detail=f"Invalid provider: {provider}")

    handler = S3Handler(provider, storage_config.local_cache_config)
    start_time = time.perf_counter()
    success, message = handler.test_connection()
    latency = (time.perf_counter() - start_time) * 1000

    return ConnectionTestResponse(
        provider=provider,
        success=success,
        message=message,
        latency_ms=latency if success else None
    )


# ============== Upload Routes ==============

@upload_router.post("/image", response_model=ImageUploadResponse)
async def upload_image(
    file: UploadFile = File(...),
    custom_caption: str = Form(default="")
):
    """Upload and analyze an image."""
    # Validate file type
    allowed_types = ['image/jpeg', 'image/png', 'image/webp', 'image/gif', 'image/bmp']
    if file.content_type not in allowed_types:
        raise HTTPException(status_code=400, detail=f"Invalid file type. Allowed: {allowed_types}")

    try:
        # Read and analyze image
        content = await file.read()
        img = Image.open(io.BytesIO(content)).convert('RGB')

        # Get image analyzer
        analyzer = get_image_analyzer()
        analysis = analyzer.analyze(img)

        caption = custom_caption if custom_caption else analysis.get('caption', '')

        # Create metadata
        metadata = {
            'modality': 'image',
            'caption': sanitize_for_s3_metadata(caption),
            'detected_objects': sanitize_for_s3_metadata(analysis.get('detected_objects', '')),
            'width': str(analysis.get('width', 0)),
            'height': str(analysis.get('height', 0)),
            'upload_time': datetime.now().isoformat(),
            'has_embedding': str(analysis.get('embedding_dims', 0) > 0).lower()
        }

        # Upload to storage
        handler = S3Handler('ddn_infinia')
        filename = Path(file.filename).name
        object_key = f"images/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{filename}"

        success, msg = handler.upload_bytes(content, object_key, metadata, file.content_type)

        if not success:
            raise HTTPException(status_code=500, detail=msg)

        return ImageUploadResponse(
            success=True,
            message="Image uploaded successfully",
            object_key=object_key,
            caption=caption,
            detected_objects=analysis.get('detected_objects', ''),
            width=analysis.get('width', 0),
            height=analysis.get('height', 0),
            has_embedding=analysis.get('embedding_dims', 0) > 0
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@upload_router.post("/video", response_model=VideoUploadResponse)
async def upload_video(
    file: UploadFile = File(...),
    custom_summary: str = Form(default=""),
    custom_tags: str = Form(default="")
):
    """Upload and analyze a video."""
    # Validate file type
    allowed_types = ['video/mp4', 'video/avi', 'video/mov', 'video/quicktime', 'video/webm', 'video/x-msvideo']
    if file.content_type not in allowed_types:
        raise HTTPException(status_code=400, detail=f"Invalid file type. Allowed: {allowed_types}")

    try:
        # Save to temp file for processing
        content = await file.read()
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file.filename).suffix) as tmp:
            tmp.write(content)
            tmp_path = tmp.name

        try:
            # Analyze video
            analyzer = get_video_analyzer()
            analysis = analyzer.analyze(tmp_path)

            summary = custom_summary if custom_summary else analysis.get('summary', '')
            tags = custom_tags if custom_tags else analysis.get('detected_objects', '')

            # Create metadata
            metadata = {
                'modality': 'video',
                'summary': sanitize_for_s3_metadata(summary),
                'tags': sanitize_for_s3_metadata(tags),
                'duration_seconds': str(analysis.get('duration_seconds', 0)),
                'fps': str(analysis.get('fps', 0)),
                'width': str(analysis.get('width', 0)),
                'height': str(analysis.get('height', 0)),
                'upload_time': datetime.now().isoformat()
            }

            # Upload to storage
            handler = S3Handler('ddn_infinia')
            filename = Path(file.filename).name
            object_key = f"videos/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{filename}"

            success, msg = handler.upload_bytes(content, object_key, metadata, file.content_type)

            if not success:
                raise HTTPException(status_code=500, detail=msg)

            # Generate presigned URL for playback
            presigned_url = handler.generate_presigned_url(object_key)

            return VideoUploadResponse(
                success=True,
                message="Video uploaded successfully",
                object_key=object_key,
                summary=summary,
                duration_seconds=analysis.get('duration_seconds', 0),
                detected_objects=tags,
                frame_count=analysis.get('total_frames', 0),
                presigned_url=presigned_url  # Add presigned URL
            )
        finally:
            os.unlink(tmp_path)

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@upload_router.post("/document", response_model=DocumentUploadResponse)
async def upload_document(
    file: UploadFile = File(...),
    custom_description: str = Form(default="")
):
    """Upload and analyze a document."""
    # Validate file type
    allowed_extensions = ['.pdf', '.docx', '.doc', '.txt']
    file_ext = Path(file.filename).suffix.lower()
    if file_ext not in allowed_extensions:
        raise HTTPException(status_code=400, detail=f"Invalid file type. Allowed: {allowed_extensions}")

    try:
        # Save to temp file for processing
        content = await file.read()
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:
            tmp.write(content)
            tmp_path = tmp.name

        try:
            # Analyze document
            analyzer = get_document_analyzer()
            analysis = analyzer.analyze(tmp_path)

            summary = custom_description if custom_description else analysis.get('summary', '')

            # Create metadata
            metadata = {
                'modality': 'document',
                'summary': sanitize_for_s3_metadata(summary),
                'key_terms': sanitize_for_s3_metadata(analysis.get('key_terms', '')),
                'word_count': str(analysis.get('word_count', 0)),
                'upload_time': datetime.now().isoformat()
            }

            # Upload to storage
            handler = S3Handler('ddn_infinia')
            filename = Path(file.filename).name
            object_key = f"documents/{datetime.now().strftime('%Y%m%d_%H%M%S')}_{filename}"

            # Determine content type
            content_type = 'application/pdf' if file_ext == '.pdf' else 'application/octet-stream'

            success, msg = handler.upload_bytes(content, object_key, metadata, content_type)

            if not success:
                raise HTTPException(status_code=500, detail=msg)

            return DocumentUploadResponse(
                success=True,
                message="Document uploaded successfully",
                object_key=object_key,
                summary=summary,
                word_count=analysis.get('word_count', 0),
                key_terms=analysis.get('key_terms', '')
            )
        finally:
            os.unlink(tmp_path)

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============== Search Routes ==============

@search_router.post("/", response_model=SearchResponse)
async def semantic_search(request: SearchRequest):
    """Perform semantic search across stored content."""
    start_time = time.perf_counter()

    try:
        handler = S3Handler('ddn_infinia')
        objects, msg = handler.list_objects(include_metadata=True)
        
        # Filter out .json files (internal metadata)
        objects = [obj for obj in objects if not obj['key'].endswith('.json')]

        if not objects:
            return SearchResponse(
                success=True,
                query=request.query,
                total_results=0,
                results=[],
                search_time_ms=(time.perf_counter() - start_time) * 1000
            )

        query_lower = request.query.lower()
        results = []

        for obj in objects:
            metadata = obj.get('metadata', {})
            modality = metadata.get('modality', 'unknown')
            
            # Infer modality from file path if unknown
            if modality == 'unknown':
                object_key = obj['key']
                if object_key.startswith('images/') or object_key.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                    modality = 'image'
                elif object_key.startswith('videos/') or object_key.lower().endswith(('.mp4', '.avi', '.mov', '.webm')):
                    modality = 'video'
                elif object_key.startswith('documents/') or object_key.lower().endswith(('.pdf', '.txt', '.docx')):
                    modality = 'document'

            # Apply modality filter
            if request.modality != "all" and modality != request.modality:
                continue

            # Calculate relevance score based on metadata match
            searchable_text = ' '.join([
                metadata.get('caption', ''),
                metadata.get('tags', ''),
                metadata.get('summary', ''),
                metadata.get('detected_objects', ''),
                metadata.get('key_terms', ''),
                obj['key']
            ]).lower()

            # Simple keyword matching
            query_words = query_lower.split()
            matches = sum(1 for word in query_words if word in searchable_text)

            if matches > 0:
                score = matches / len(query_words)

                # Generate presigned URL
                presigned_url = handler.generate_presigned_url(obj['key'])

                results.append(SearchResult(
                    object_key=obj['key'],
                    modality=modality,
                    relevance_score=score,
                    metadata=metadata,
                    size_bytes=obj.get('size', 0),
                    last_modified=obj.get('last_modified', ''),
                    presigned_url=presigned_url
                ))

        # Sort by relevance
        results.sort(key=lambda x: x.relevance_score, reverse=True)
        results = results[:request.top_k]

        return SearchResponse(
            success=True,
            query=request.query,
            total_results=len(results),
            results=results,
            search_time_ms=(time.perf_counter() - start_time) * 1000
        )

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"Search error: {e}")
        logger.error(f"Traceback:\n{error_details}")
        raise HTTPException(status_code=500, detail=f"{str(e)}\n\nTraceback: {error_details}")


# ============== Browse Routes ==============

@browse_router.post("/", response_model=BrowseResponse)
async def browse_objects(request: BrowseRequest):
    """Browse all stored content."""
    try:
        # Get all objects
        handler = S3Handler('ddn_infinia')
        all_objects, _ = handler.list_objects(prefix=request.prefix)
        
        # Filter out .json files (internal metadata)
        all_objects = [obj for obj in all_objects if not obj['key'].endswith('.json')]
        
        # Build response objects
        results = []
        for obj in all_objects:
            # Fetch metadata for each object individually to ensure we get the latest
            obj_metadata, _ = handler.get_object_metadata(obj['key'])
            metadata = obj_metadata.get('metadata', {}) if obj_metadata else {}
            
            # Infer modality from file path if not in metadata or if unknown
            modality = metadata.get('modality', 'unknown')
            if modality == 'unknown':
                object_key = obj['key']
                if object_key.startswith('images/') or object_key.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                    modality = 'image'
                elif object_key.startswith('videos/') or object_key.lower().endswith(('.mp4', '.avi', '.mov', '.webm')):
                    modality = 'video'
                elif object_key.startswith('documents/') or object_key.lower().endswith(('.pdf', '.txt', '.docx')):
                    modality = 'document'
            
            # Apply modality filter
            if request.modality != "all" and modality != request.modality:
                continue

            # Generate presigned URL
            presigned_url = handler.generate_presigned_url(obj['key'])

            results.append(ObjectInfo(
                key=obj['key'],
                modality=modality,
                size_bytes=obj.get('size', 0),
                last_modified=obj.get('last_modified', ''),
                metadata=metadata,
                presigned_url=presigned_url
            ))

        return BrowseResponse(
            success=True,
            total_objects=len(results),
            objects=results
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============== List Videos Route ==============

@browse_router.get("/videos", response_model=BrowseResponse)
async def list_videos():
    """List all videos in storage for dropdown selection."""
    try:
        handler = S3Handler('ddn_infinia')
        objects, msg = handler.list_objects(prefix='videos/')
        
        # Filter out .json files (internal metadata)
        objects = [obj for obj in objects if not obj['key'].endswith('.json')]
        
        objects_result = []
        for obj in objects:
            metadata = obj.get('metadata', {})
            modality = metadata.get('modality', 'unknown')
            
            # If modality is unknown, try to infer from object key path
            if modality == 'unknown':
                object_key = obj.get('key', '')
                if object_key.startswith('images/') or object_key.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
                    modality = 'image'
                elif object_key.startswith('videos/') or object_key.lower().endswith(('.mp4', '.avi', '.mov', '.webm')):
                    modality = 'video'
                elif object_key.startswith('documents/') or object_key.lower().endswith(('.pdf', '.txt', '.docx')):
                    modality = 'document'
                # Update the metadata with inferred modality for consistency
                metadata['modality'] = modality
            
            # Only include videos
            if modality == 'video':
                presigned_url = handler.generate_presigned_url(obj['key'])
                
                objects_result.append(ObjectInfo(
                    key=obj['key'],
                    modality=modality,
                    size_bytes=obj['size'],
                    last_modified=obj['last_modified'],
                    metadata=metadata,
                    presigned_url=presigned_url
                ))
        
        return BrowseResponse(
            success=True,
            total_objects=len(objects_result),
            objects=objects_result
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============== Video Streaming Proxy Route ==============

@browse_router.get("/video-stream/{object_key:path}")
async def stream_video(object_key: str, download: bool = False):
    """Stream video through backend to avoid browser certificate issues."""
    try:
        handler = S3Handler('ddn_infinia')
        video_bytes, msg = handler.download_bytes(object_key)
        
        if not video_bytes:
            raise HTTPException(status_code=404, detail="Video not found")
        
        # Determine content type
        if object_key.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):
            media_type = 'image/jpeg'
        elif object_key.lower().endswith(('.mp4', '.avi', '.mov', '.webm')):
            media_type = 'video/mp4'
        else:
            media_type = 'application/octet-stream'
        
        # Return with proper headers
        from fastapi.responses import Response
        headers = {
            "Accept-Ranges": "bytes",
            "Content-Length": str(len(video_bytes)),
        }
        
        # Add download header if requested
        if download:
            filename = object_key.split('/')[-1]
            headers["Content-Disposition"] = f'attachment; filename="{filename}"'
        
        return Response(
            content=video_bytes,
            media_type=media_type,
            headers=headers
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============== Copy Object Route ==============

@browse_router.post("/copy")
async def copy_object(source_key: str, destination_key: str):
    """Copy an object to a new location."""
    try:
        handler = S3Handler('ddn_infinia')
        success, message = handler.copy_object(source_key, destination_key)
        
        if not success:
            raise HTTPException(status_code=500, detail=message)
        
        return {"success": True, "message": message}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============== Move Object Route ==============

@browse_router.post("/move")
async def move_object(source_key: str, destination_key: str):
    """Move an object to a new location (copy + delete)."""
    try:
        handler = S3Handler('ddn_infinia')
        success, message = handler.move_object(source_key, destination_key)
        
        if not success:
            raise HTTPException(status_code=500, detail=message)
        
        return {"success": True, "message": message}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@browse_router.delete("/{object_key:path}")
async def delete_object(object_key: str):
    """Delete an object from storage."""
    try:
        handler = S3Handler('ddn_infinia')
        success, msg = handler.delete_object(object_key)

        if not success:
            raise HTTPException(status_code=500, detail=msg)

        return {"success": True, "message": f"Deleted: {object_key}"}

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============== Health Routes ==============

@health_router.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint."""
    image_analyzer = get_image_analyzer()

    return HealthResponse(
        status="healthy",
        ddn_configured=bool(storage_config.ddn_infinia_config.get('access_key')),
        aws_configured=bool(storage_config.aws_config.get('access_key')),
        ai_models_loaded=image_analyzer.models_loaded if image_analyzer else False,
        gpu_available=TORCH_AVAILABLE and DEVICE == "cuda",
        device=DEVICE
    )


@health_router.get("/metrics", response_model=MetricsResponse)
async def get_metrics():
    """Get storage metrics."""
    try:
        handler = S3Handler('ddn_infinia')
        objects, _ = handler.list_objects()

        total_images = 0
        total_videos = 0
        total_documents = 0
        total_bytes = 0

        for obj in objects:
            metadata = obj.get('metadata', {})
            modality = metadata.get('modality', 'unknown')
            total_bytes += obj.get('size', 0)

            if modality == 'image':
                total_images += 1
            elif modality == 'video':
                total_videos += 1
            elif modality == 'document':
                total_documents += 1

        gpu_memory = None
        if TORCH_AVAILABLE and DEVICE == "cuda":
            import torch
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024

        return MetricsResponse(
            total_images=total_images,
            total_videos=total_videos,
            total_documents=total_documents,
            total_storage_bytes=total_bytes,
            gpu_memory_used_mb=gpu_memory
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============== Video Frame Search Route ==============

@upload_router.post("/video/search-frames", response_model=VideoFrameSearchResponse)
async def search_video_frames(request: VideoFrameSearchRequest):
    """Search video frames using semantic similarity with CLIP."""
    try:
        # Download video from storage
        handler = S3Handler('ddn_infinia')
        video_bytes, msg = handler.download_bytes(request.video_key)
        
        if not video_bytes:
            raise HTTPException(status_code=404, detail=f"Video not found: {msg}")
        
        # Save to temp file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp:
            tmp.write(video_bytes)
            tmp_path = tmp.name
        
        try:
            # Perform semantic frame search
            analyzer = get_video_analyzer()
            matching_frames = analyzer.search_frames_semantic(
                tmp_path,
                request.query,
                request.threshold
            )
            
            return VideoFrameSearchResponse(
                success=True,
                video_key=request.video_key,
                query=request.query,
                matching_frames=matching_frames,
                total_frames_analyzed=20  # We extract 20 keyframes
            )
        finally:
            os.unlink(tmp_path)
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
# ============== Continuous Ingestion Routes ==============

@ingestion_router.post("/start")
async def start_bucket_monitoring(bucket_name: str):
    """Start monitoring S3 bucket for new files in auto_ingest folder."""
    logger.info(f"ðŸ“¡ Starting bucket monitoring for: {bucket_name}")
    try:
        message = bucket_monitor.start_monitoring(bucket_name)
        return {"success": True, "message": message, "bucket_name": bucket_name}
    except Exception as e:
        logger.error(f"Failed to start monitoring: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@ingestion_router.post("/stop")
async def stop_bucket_monitoring():
    """Stop bucket monitoring."""
    logger.info("ðŸ›‘ Stopping bucket monitoring")
    try:
        message = bucket_monitor.stop_monitoring()
        return {"success": True, "message": message}
    except Exception as e:
        logger.error(f"Failed to stop monitoring: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@ingestion_router.get("/status")
async def get_monitoring_status():
    """Get current bucket monitoring status."""
    try:
        status = bucket_monitor.get_status()
        return status
    except Exception as e:
        logger.error(f"Failed to get status: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@ingestion_router.get("/stream")
async def stream_processing_events():
    """Stream real-time processing events via SSE."""
    logger.info("ðŸ“¡ Client connected to SSE stream")
    return StreamingResponse(
        bucket_monitor.stream_events(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )
